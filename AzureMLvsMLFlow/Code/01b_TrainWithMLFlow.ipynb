{"cells":[{"cell_type":"code","source":["import mlflow\nfrom mlflow.tracking import MlflowClient\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import Pipeline\nimport datetime as dt\nfrom pyspark.ml.feature import OneHotEncoder, VectorAssembler\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder \nimport mlflow.mleap"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%scala\nval tags = com.databricks.logging.AttributionContext.current.tags\nval username = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_USER, java.util.UUID.randomUUID.toString.replace(\"-\", \"\"))\nspark.conf.set(\"com.databricks.demo.username\", username)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["client = MlflowClient() # client\nexps = client.list_experiments() # get all experiments"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["exps"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["exp = [s for s in exps if \"/Users/{}/exps/MLFlowExp\".format(spark.conf.get(\"com.databricks.demo.username\")) in s.name][0] # get only the exp we want\nexp_id = exp.experiment_id # save exp id to variable\nartifact_location = exp.artifact_location # artifact location for storing\nrun = client.create_run(exp_id) # create the run\nrun_id = run.info.run_id # get the run id"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# start and mlflow run\nmlflow.start_run(run_id)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df = (spark\n      .read\n      .format(\"csv\")\n      .option(\"inferSchema\", \"True\")\n      .option(\"header\", \"True\")\n      .load(\"/databricks-datasets/bikeSharing/data-001/day.csv\")\n     )\n# split data\ntrain_df, test_df = df.randomSplit([0.7, 0.3])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# One Hot Encoding\nmnth_encoder = OneHotEncoder(inputCol=\"mnth\", outputCol=\"encoded_mnth\")\nweekday_encoder = OneHotEncoder(inputCol=\"weekday\", outputCol=\"encoded_weekday\")\n\n# set the training variables we want to use\ntrain_cols = ['encoded_mnth', 'encoded_weekday', 'temp', 'hum']\n\n# convert cols to a single features col\nassembler = VectorAssembler(inputCols=train_cols, outputCol=\"features\")\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"cnt\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Create pipeline\npipeline = Pipeline(stages=[\n    mnth_encoder,\n    weekday_encoder,\n    assembler,\n    dt\n])"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["grid = (ParamGridBuilder()\n  .addGrid(dt.maxDepth, [2, 3, 4, 5, 6, 7, 8])\n  .addGrid(dt.maxBins, [2, 4, 8])\n  .build())"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["valid_eval =  RegressionEvaluator(labelCol = \"cnt\", predictionCol = \"prediction\", metricName=\"rmse\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["cv = CrossValidator(estimator=pipeline, evaluator=valid_eval, estimatorParamMaps=grid, numFolds=3)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["cvModel = cv.fit(train_df)\nmlflow.set_tag('owner_team', spark.conf.get(\"com.databricks.demo.username\")) # Logs user-defined tags\ntest_metric = valid_eval.evaluate(cvModel.transform(test_df))\nmlflow.log_metric('test_' + valid_eval.getMetricName(), test_metric) # Logs additional metrics\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["bestModel = cvModel.bestModel\n# write test predictions to datetime and lastest folder\npredictions = bestModel.transform(test_df)\n# mlflow log evaluations\nevaluator = RegressionEvaluator(labelCol = \"cnt\", predictionCol = \"prediction\")\n\nmlflow.log_metric(\"mae\", evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"}))\nmlflow.log_metric(\"rmse\", evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"}))\nmlflow.log_metric(\"r2\", evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"}))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["lrPipelineModel.write().overwrite().save(\"{}/latest/bike_sharing_model.model\".format(artifact_location))\nlrPipelineModel.write().overwrite().save(\"{}/year={}/month={}/day={}/bike_sharing_model.model\".format(artifact_location, dt.datetime.utcnow().year, dt.datetime.utcnow().month, dt.datetime.utcnow().day))\n\n# write test predictions to datetime and lastest folder\n\npredictions.write.format(\"parquet\").mode(\"overwrite\").save(\"{}/latest/test_predictions.parquet\".format(artifact_location))\npredictions.write.format(\"parquet\").mode(\"overwrite\").save(\"{}/year={}/month={}/day={}/test_predictions.parquet\".format(artifact_location, dt.datetime.utcnow().year, dt.datetime.utcnow().month, dt.datetime.utcnow().day))\nmlflow.set_tag(\"Model Path\", \"{}/year={}/month={}/day={}\".format(artifact_location, dt.datetime.utcnow().year, dt.datetime.utcnow().month, dt.datetime.utcnow().day))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["mlflow.end_run(status=\"FINISHED\")"],"metadata":{},"outputs":[],"execution_count":18}],"metadata":{"name":"01b_TrainWithMLFlow","notebookId":285567593352821},"nbformat":4,"nbformat_minor":0}
